{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exam 1\n",
    "## Applied Machine Learning (BUAN 6341)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Deliverables:\n",
    "\n",
    "- Write down your answers to MCQ in the answer sheet.\n",
    "- Write down your codes, printed results, and corresponding discussions in the cells below the problems.\n",
    "    - You will find several cells under each problem. You can use one, some, or all cells based on your preference.\n",
    "    - If you need more cells to check temporary results, add a cell using the \"+\" button or shortcut \"esc + b\". You do NOT need to remove these cells upon submission.\n",
    "- Include TWO files (.ipynb, and .docx) in a zipped folder and submit to eLearning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "NAME = \"Prachi Dalal\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline \n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import sklearn\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Section 2.1. Regression Models\n",
    "\n",
    "In this section, we are going to use the same bike-sharing dataset in Assignment 1. Below is a brief description of the dataset (also same as Assignment 1).\n",
    "\n",
    "##### The Dataset\n",
    "\n",
    "We will be using the daily version of the Capital Bikeshare System dataset from the UCI Machine Learning Repository. This data set contains information about the daily count of bike rental checkouts in Washington, D.C.’s bikeshare program between 2011 and 2012. It also includes information about the weather and seasonal/temporal features for that day (like whether it was a weekday).\n",
    "- **day:** Day of the record (relative to day 1:2011-01-01)\n",
    "- **season:** Season (1:spring, 2:summer, 3:fall, 4:winter)\n",
    "- **weekday:** Day of the week (0=Sunday, 6=Saturday)\n",
    "- **workingday:** If day is neither weekend nor holiday is 1, otherwise is 0.\n",
    "- **weathersit:**<br>\n",
    "    - 1: Clear, Few clouds, Partly cloudy, Partly cloudy\n",
    "    - 2: Mist + Cloudy, Mist + Broken clouds, Mist + Few clouds, Mist\n",
    "    - 3: Light Snow, Light Rain + Thunderstorm + Scattered clouds, Light Rain + Scattered\n",
    "clouds\n",
    "- **temp:** Normalized temperature in Celcius\n",
    "- **windspeed:** Normalized wind speed\n",
    "- **casual:** Count of checkouts by casual/non-registered users\n",
    "- **registered:** Count of checkouts by registered users\n",
    "- **cnt:** Total checkouts\n",
    "\n",
    "#### Data Prep\n",
    "\n",
    "Run the cells below to load the split the data. We will use features ('weekday', 'season','workingday', 'temp', 'windspeed', 'weathersit') to predict the checkout counts. We are interested in predicting the total checkout counts (i.e., 'cnt').\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['day' 'season' 'weekday' 'workingday' 'weathersit' 'temp' 'windspeed'\n",
      " 'casual' 'registered' 'cnt']\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>day</th>\n",
       "      <th>season</th>\n",
       "      <th>weekday</th>\n",
       "      <th>workingday</th>\n",
       "      <th>weathersit</th>\n",
       "      <th>temp</th>\n",
       "      <th>windspeed</th>\n",
       "      <th>casual</th>\n",
       "      <th>registered</th>\n",
       "      <th>cnt</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0.344167</td>\n",
       "      <td>0.160446</td>\n",
       "      <td>331</td>\n",
       "      <td>654</td>\n",
       "      <td>985</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0.363478</td>\n",
       "      <td>0.248539</td>\n",
       "      <td>131</td>\n",
       "      <td>670</td>\n",
       "      <td>801</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.196364</td>\n",
       "      <td>0.248309</td>\n",
       "      <td>120</td>\n",
       "      <td>1229</td>\n",
       "      <td>1349</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>0.160296</td>\n",
       "      <td>108</td>\n",
       "      <td>1454</td>\n",
       "      <td>1562</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.226957</td>\n",
       "      <td>0.186900</td>\n",
       "      <td>82</td>\n",
       "      <td>1518</td>\n",
       "      <td>1600</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   day  season  weekday  workingday  weathersit      temp  windspeed  casual  \\\n",
       "0    1       1        6           0           2  0.344167   0.160446     331   \n",
       "1    2       1        0           0           2  0.363478   0.248539     131   \n",
       "2    3       1        1           1           1  0.196364   0.248309     120   \n",
       "3    4       1        2           1           1  0.200000   0.160296     108   \n",
       "4    5       1        3           1           1  0.226957   0.186900      82   \n",
       "\n",
       "   registered   cnt  \n",
       "0         654   985  \n",
       "1         670   801  \n",
       "2        1229  1349  \n",
       "3        1454  1562  \n",
       "4        1518  1600  "
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "day = pd.read_csv(\"day.csv\")\n",
    "print(day.columns.values) # to find variable names\n",
    "day.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "var = ['weekday', 'season','workingday', 'temp', 'windspeed', 'weathersit']\n",
    "X = day[var]\n",
    "y = day['cnt']\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Problem 1: Linear Regression (10 pts)**\n",
    "\n",
    "Train a Linear regression model using the training set. \n",
    "\n",
    "- We would like to use MSE as the performance measure. What is the MSE for the training and the test set?  Which value should be reported to evaluate the model's performance? Explain briefly.\n",
    "- Report ONLY the coefficients for variable \"temp\" and \"weathersit\". Match the coefficient with the variable properly. How would you interpret the coefficients for the two variables? *(Note: You need only discuss the positive/negative relationship between the X and Y. You do not need to interpret the explicit magnitude.)*\n",
    "    - Do you think the results make sense? Briefly explain.\n",
    "    - Do you think we are coding the two variables (i.e., temp and weathersit) properly? Which one(s) do you think is improper? Do you have suggestions for improvement? (Discussion Only. Code not required).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import \n",
    "from sklearn.linear_model import LinearRegression\n",
    "model = LinearRegression()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LinearRegression()"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#train model\n",
    "model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "#(1) y_pred_train\n",
    "y_pred_train = model.predict(X_train)\n",
    "\n",
    "#(2) y_pred_test\n",
    "y_pred_test = model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE of training: 1852849.4373569074\n",
      "MSE of test: 1816812.3742340647\n"
     ]
    }
   ],
   "source": [
    "#performance evaluation \n",
    "#\"What is the MSE for the training and the test set?\"\n",
    "#test\n",
    "etest = y_test - y_pred_test # error\n",
    "MSE_test = np.mean(etest**2)\n",
    "\n",
    "#training \n",
    "etrain = y_train - y_pred_train \n",
    "MSE_train = np.mean(etrain**2)\n",
    "print(\"MSE of training:\", MSE_train)\n",
    "print(\"MSE of test:\", MSE_test)\n",
    "\n",
    "#The MSE value for the test set should be reported because it is an unbiased measure since it was not used during parameter\n",
    "#estimation / training "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "coefficients:\n",
      "temperature: 5245.485671894189\n",
      "weathersit -800.6094377679341\n"
     ]
    }
   ],
   "source": [
    "#Report ONLY the coefficients for variable \"temp\" and \"weathersit\". Match the coefficient with the variable properly. \n",
    "np.set_printoptions(precision=4, suppress=True)\n",
    "var = ['weekday', 'season','workingday', 'temp', 'windspeed', 'weathersit']\n",
    "print(\"coefficients:\")\n",
    "print(\"temperature:\", model.coef_[3])\n",
    "print(\"weathersit\", model.coef_[5])\n",
    "\n",
    "#How would you interpret the coefficients for the two variables? (Note: You need only discuss the positive/negative \n",
    "#relationship between the X and Y. You do not need to interpret the explicit magnitude.)\n",
    "#the temperature variable has a positive relationship with the dependent variable y. As temperature increases, the number of bike rentals increases. \n",
    "#the weathersit variable has a negative relationship with the dependent variable y. As weathersit increases, the number of bike rentals descreases. \n",
    "\n",
    "#Do you think the results make sense? Briefly explain.\n",
    "#Yes these results make sense because in general, people want to bike when the temperature is hotter. It also makes sense that as weathersit increases, bike rentals descrease because the highest category of weathersit is bad weather. \n",
    "\n",
    "#Do you think we are coding the two variables (i.e., temp and weathersit) properly? Which one(s) do you think is \n",
    "#improper? Do you have suggestions for improvement? \n",
    "#it is a possibility that we are not coding weathersit properly. Since it is a categorical variable (1,2,3), \n",
    "#we may need to normalize the data to ensure all the variables are on the same scale. \n",
    "#We can use min/max scalar to ensure all the variables are on the same scale and the model is measuring them equally. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Problem 2: LASSO Regression (12 pts)**\n",
    "\n",
    "Train a LASSO regression model with grid search and 5-fold cross validation. The potential choices of lambda (i.e., hyperparameters) are: 0.001, 0.01, 0.1, 1, 10, 100.  \n",
    "\n",
    "- What is the hyperparameter chosen? \n",
    "- Report the coefficient and intercept of the best model chosen. \n",
    "- Based on the current coefficient estimates, without training additional models, answer questions below.\n",
    "    - Are there any variables that are not helpful in predicting the total checkout count? If yes, what are they? Explain briefly.\n",
    "    - If we set lambda to 0.0001, are we able to exclude any variables? If we set lambda to 1000, are we able to exclude any variables? Explain briefly.\n",
    "- We would like to use R-squared as the performance measure. What is the R-squared value for the test set?  What is the mean validation R-squared value of the best model? \n",
    "- The two measures (i.e., mean validation R-squared value and test R-squared value) are used for different purposes. Briefly explain what the two measures are used for correspondingly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import\n",
    "from sklearn.linear_model import Lasso\n",
    "from sklearn.model_selection import GridSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define grid of parameters\n",
    "lasso_params = { 'alpha' :  [0.001, 0.01, 0.1, 1, 10, 100] }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "#define gridsearch estimation function \n",
    "lasso = Lasso()\n",
    "lasso_grid = GridSearchCV(lasso, lasso_params, cv = 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=5, estimator=Lasso(),\n",
       "             param_grid={'alpha': [0.001, 0.01, 0.1, 1, 10, 100]})"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#train data \n",
    "lasso_grid.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'alpha': 1}\n"
     ]
    }
   ],
   "source": [
    "#\"What is the hyperparameter chosen?\" - alpha = 1\n",
    "#best hyperparameter\n",
    "print(lasso_grid.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "coefficient of best model: [   76.5378   415.7713   128.731   5222.2339 -1824.4194  -799.342 ]\n",
      "intercept of best model 2066.023470990603\n"
     ]
    }
   ],
   "source": [
    "#Report the coefficient and intercept of the best model chosen.\n",
    "print(\"coefficient of best model:\", lasso_grid.best_estimator_.coef_)\n",
    "print(\"intercept of best model\", lasso_grid.best_estimator_.intercept_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Are there any variables that are not helpful in predicting the total checkout count? \n",
    "#If yes, what are they? Explain briefly.\n",
    "var = ['weekday', 'season','workingday', 'temp', 'windspeed', 'weathersit']\n",
    "#LASSO punishes variables by making their coefficients 0. Since none of these coefficients are close to 0, \n",
    "#we can say they are all helpful in predicting the total checkout count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "alpha = 0.001: [   76.774    412.6659   132.2063  5245.4624 -1995.5644  -800.6082] 2093.381363143727\n",
      "alpha = 1000: [ 0.  0.  0.  0. -0. -0.] 4574.998175182482\n"
     ]
    }
   ],
   "source": [
    "#If we set lambda to 0.0001, are we able to exclude any variables?\n",
    "#If we set lambda to 1000, are we able to exclude any variables? Explain briefly.\n",
    "from sklearn.linear_model import Lasso\n",
    "\n",
    "lasso_base1 = Lasso(alpha = 0.001)\n",
    "lasso_base1.fit(X_train, y_train)\n",
    "y_pred_test = lasso_base1.predict(X_test)\n",
    "print(\"alpha = 0.001:\", lasso_base1.coef_, lasso_base1.intercept_) \n",
    "\n",
    "lasso_base2 = Lasso(alpha = 1000)\n",
    "lasso_base2.fit(X_train, y_train)\n",
    "y_pred_test = lasso_base2.predict(X_test)\n",
    "print(\"alpha = 1000:\", lasso_base2.coef_, lasso_base2.intercept_) \n",
    "\n",
    "#Yes. In the first model with alpha = 0.001, none of the coefficients are close to 0 so we cannot exclude any of them\n",
    "#In the second model, however, we can exclude all of the variables. In LASSO regression, as alpha reaches infinity, the coefficients get set to 0 since it is a more conservative model \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5273339521987706\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.4761310883810908"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#We would like to use R-squared as the performance measure. \n",
    "#What is the R-squared value for the test set? \n",
    "print(lasso_grid.score(X_test, y_test))\n",
    "\n",
    "#What is the mean validation R-squared value of the best model?\n",
    "np.max(lasso_grid.cv_results_['mean_test_score'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "#The two measures (i.e., mean validation R-squared value and test R-squared value) are used for different purposes. Briefly explain what the\n",
    "#two measures are used for correspondingly.\n",
    "#the test R^2 value is the unbiased performance measure for the test set.\n",
    "#On the other hand, the mean validation R-sqaured value for the best model is the performance measure based on which \"best\" hyperparamer is chosen (R2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Problem 3: Ridge Regression Extended (12 pts)**\n",
    "\n",
    "Train a Ridge regression model, ridge50, using the training set with lambda = 50. Then Train another Ridge regression model, ridge5, using the training set with lambda = 5.\n",
    "\n",
    "- Report the coefficients and the intercept of the two Ridge regressions. \n",
    "- Compare the coefficients and intercept of the two Ridge regression. What pattern do you expect when comparing the coefficients and intercept of the two models?\n",
    "    - Is the comparison of the two intercepts and all coefficients consistent with your expectation? \n",
    "    - Do you have any guesses on why the pattern is/is not consistent with your expectation?\n",
    "- Now, obtain the sum of the squared value of the coefficients for (1) the ridge50 model, and (2) the ridge5 model. Then add the squared value of the intercept of the corresponding models. Here, you have obtained two summed values.\n",
    "    - Report and compare the two values, what do you find? \n",
    "    - Based on the practice so far, as well as the penalty term applied to the objective function, can you briefly explain what coefficient pattern is explicitly expected when comparing a Ridge regression with different lambdas?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "coefficients: [  77.6418  616.0391  163.959  1317.6573 -178.9279 -750.325 ]\n",
      "intercept: 3115.1364365160916\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import Ridge\n",
    "ridge50 = Ridge(alpha = 50)\n",
    "ridge50.fit(X_train, y_train)\n",
    "y_pred_test = ridge50.predict(X_test)\n",
    "\n",
    "print(\"coefficients (50):\", ridge50.coef_)\n",
    "print(\"intercept:\",  ridge50.intercept_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "coefficients (5): [  77.3541  495.8282  161.8522 4022.3383 -863.8155 -837.4364]\n",
      "intercept: 2314.03990678701\n"
     ]
    }
   ],
   "source": [
    "ridge5 = Ridge(alpha = 5)\n",
    "ridge5.fit(X_train, y_train)\n",
    "y_pred_test = ridge5.predict(X_test)\n",
    "\n",
    "print(\"coefficients (5):\", ridge5.coef_)\n",
    "print(\"intercept:\",  ridge5.intercept_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Compare the coefficients and intercept of the two Ridge regression. \n",
    "#What pattern do you expect when comparing the coefficients and intercept of the two models?\n",
    "# I expect to see lower coefficients for a higher alpha because since the model is being more conservative, the coefficients that aren't as relevant get smaller and have a lower impact. \n",
    "\n",
    "#Is the comparison of the two intercepts and all coefficients consistent \n",
    "#with your expectation?\n",
    "#yes, although majority of the coefficients (when alpha is 5) are lower than the coefficients when alpha is 50, the last two coefficients decreased significantly, having an overall effect on the model. \n",
    "#this shows that increases alpha increases how strict the model is, and will lower the coefficient's impact as a result. \n",
    "\n",
    "#Do you have any guesses on why the pattern is/is \n",
    "#not consistent with your expectation?\n",
    "#When the coefficients of the variables decrease, it causes them to have less of an impact on the dependent variable. Alpha acts as the shinkage penalty so the higher the alpha, the higher the penalty the coefficients get. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Now, obtain the sum of the squared value of the coefficients for (1) the ridge50 model, and (2) the ridge5 model. Then add the squared value of the intercept of the corresponding models. \n",
    "#Here, you have obtained two summed values.\n",
    "\n",
    "\n",
    "#Report and compare the two values, what do you find?\n",
    "#Based on the practice so far, as well as the penalty term applied to the objective function, can you briefly explain what coefficient pattern is explicitly expected when comparing a Ridge regression with different lambdas?\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Section 2.2 Classification\n",
    "\n",
    "In this section, we would like to study the taste preference of red wine. You need to train different classification models and answer corresponding questions.\n",
    "\n",
    "#### Data and Setting\n",
    "\n",
    "We consider *vinho verde*, a unique product from the Minho (northwest) region of Portugal. Testers are invited to taste the wine and rate the wine quality. The detailed information of wine can be found in “wine.csv”. Variables are listed below. (You can skip the variable description as they are mostly area-specific terminologies).\n",
    "\n",
    "\n",
    "- Fixed acidity (g(tartaric acid)/dm3)\n",
    "- Volatile acidity (g(acetic acid)/dm3)\n",
    "- Citric acid (g/dm3)\n",
    "- Residual sugar (g/dm3)\n",
    "- Chlorides (g(sodium chloride)/dm3)\n",
    "- Free sulfur dioxide (mg/dm3)\n",
    "- Total sulfur dioxide (mg/dm3)\n",
    "- Density (g/cm3)\n",
    "- pH\n",
    "- Sulphates (g(potassium sulphate)/dm3)\n",
    "- Alcohol (vol.%)\n",
    "- Quality: wine quality rated by tester\n",
    "\n",
    "#### Data Preparation\n",
    "We use all other variables to predict the wine quality (Quality). Use the code below to load and split the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['fixedacidity' 'volatileacidity' 'citricacid' 'residualsugar' 'chlorides'\n",
      " 'freesulfurdioxide' 'totalsulfurdioxide' 'density' 'ph' 'sulphates'\n",
      " 'alcohol' 'quality']\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>fixedacidity</th>\n",
       "      <th>volatileacidity</th>\n",
       "      <th>citricacid</th>\n",
       "      <th>residualsugar</th>\n",
       "      <th>chlorides</th>\n",
       "      <th>freesulfurdioxide</th>\n",
       "      <th>totalsulfurdioxide</th>\n",
       "      <th>density</th>\n",
       "      <th>ph</th>\n",
       "      <th>sulphates</th>\n",
       "      <th>alcohol</th>\n",
       "      <th>quality</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>7.4</td>\n",
       "      <td>0.70</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1.9</td>\n",
       "      <td>0.076</td>\n",
       "      <td>11.0</td>\n",
       "      <td>34.0</td>\n",
       "      <td>0.9978</td>\n",
       "      <td>3.51</td>\n",
       "      <td>0.56</td>\n",
       "      <td>9.4</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>7.8</td>\n",
       "      <td>0.88</td>\n",
       "      <td>0.00</td>\n",
       "      <td>2.6</td>\n",
       "      <td>0.098</td>\n",
       "      <td>25.0</td>\n",
       "      <td>67.0</td>\n",
       "      <td>0.9968</td>\n",
       "      <td>3.20</td>\n",
       "      <td>0.68</td>\n",
       "      <td>9.8</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>7.8</td>\n",
       "      <td>0.76</td>\n",
       "      <td>0.04</td>\n",
       "      <td>2.3</td>\n",
       "      <td>0.092</td>\n",
       "      <td>15.0</td>\n",
       "      <td>54.0</td>\n",
       "      <td>0.9970</td>\n",
       "      <td>3.26</td>\n",
       "      <td>0.65</td>\n",
       "      <td>9.8</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>11.2</td>\n",
       "      <td>0.28</td>\n",
       "      <td>0.56</td>\n",
       "      <td>1.9</td>\n",
       "      <td>0.075</td>\n",
       "      <td>17.0</td>\n",
       "      <td>60.0</td>\n",
       "      <td>0.9980</td>\n",
       "      <td>3.16</td>\n",
       "      <td>0.58</td>\n",
       "      <td>9.8</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>7.4</td>\n",
       "      <td>0.70</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1.9</td>\n",
       "      <td>0.076</td>\n",
       "      <td>11.0</td>\n",
       "      <td>34.0</td>\n",
       "      <td>0.9978</td>\n",
       "      <td>3.51</td>\n",
       "      <td>0.56</td>\n",
       "      <td>9.4</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   fixedacidity  volatileacidity  citricacid  residualsugar  chlorides  \\\n",
       "0           7.4             0.70        0.00            1.9      0.076   \n",
       "1           7.8             0.88        0.00            2.6      0.098   \n",
       "2           7.8             0.76        0.04            2.3      0.092   \n",
       "3          11.2             0.28        0.56            1.9      0.075   \n",
       "4           7.4             0.70        0.00            1.9      0.076   \n",
       "\n",
       "   freesulfurdioxide  totalsulfurdioxide  density    ph  sulphates  alcohol  \\\n",
       "0               11.0                34.0   0.9978  3.51       0.56      9.4   \n",
       "1               25.0                67.0   0.9968  3.20       0.68      9.8   \n",
       "2               15.0                54.0   0.9970  3.26       0.65      9.8   \n",
       "3               17.0                60.0   0.9980  3.16       0.58      9.8   \n",
       "4               11.0                34.0   0.9978  3.51       0.56      9.4   \n",
       "\n",
       "   quality  \n",
       "0        0  \n",
       "1        0  \n",
       "2        0  \n",
       "3        1  \n",
       "4        0  "
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "wine = pd.read_csv(\"wine.csv\")\n",
    "print(wine.columns.values) # to find variable names\n",
    "wine.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "var = ['fixedacidity','volatileacidity','citricacid','residualsugar','chlorides','freesulfurdioxide',\\\n",
    "     'totalsulfurdioxide','density', 'ph','sulphates', 'alcohol']\n",
    "X = wine[var]\n",
    "y = wine['quality']\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=999)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the cell below to obtain summary statistics of the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>fixedacidity</th>\n",
       "      <th>volatileacidity</th>\n",
       "      <th>citricacid</th>\n",
       "      <th>residualsugar</th>\n",
       "      <th>chlorides</th>\n",
       "      <th>freesulfurdioxide</th>\n",
       "      <th>totalsulfurdioxide</th>\n",
       "      <th>density</th>\n",
       "      <th>ph</th>\n",
       "      <th>sulphates</th>\n",
       "      <th>alcohol</th>\n",
       "      <th>quality</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>1599.000000</td>\n",
       "      <td>1599.000000</td>\n",
       "      <td>1599.000000</td>\n",
       "      <td>1599.000000</td>\n",
       "      <td>1599.000000</td>\n",
       "      <td>1599.000000</td>\n",
       "      <td>1599.000000</td>\n",
       "      <td>1599.000000</td>\n",
       "      <td>1599.000000</td>\n",
       "      <td>1599.000000</td>\n",
       "      <td>1599.000000</td>\n",
       "      <td>1599.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>8.319637</td>\n",
       "      <td>0.527821</td>\n",
       "      <td>0.270976</td>\n",
       "      <td>2.538805</td>\n",
       "      <td>0.087467</td>\n",
       "      <td>15.874922</td>\n",
       "      <td>46.467792</td>\n",
       "      <td>0.996747</td>\n",
       "      <td>3.311113</td>\n",
       "      <td>0.658149</td>\n",
       "      <td>10.422983</td>\n",
       "      <td>0.534709</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>1.741096</td>\n",
       "      <td>0.179060</td>\n",
       "      <td>0.194801</td>\n",
       "      <td>1.409928</td>\n",
       "      <td>0.047065</td>\n",
       "      <td>10.460157</td>\n",
       "      <td>32.895324</td>\n",
       "      <td>0.001887</td>\n",
       "      <td>0.154386</td>\n",
       "      <td>0.169507</td>\n",
       "      <td>1.065668</td>\n",
       "      <td>0.498950</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>4.600000</td>\n",
       "      <td>0.120000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.900000</td>\n",
       "      <td>0.012000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>6.000000</td>\n",
       "      <td>0.990070</td>\n",
       "      <td>2.740000</td>\n",
       "      <td>0.330000</td>\n",
       "      <td>8.400000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>7.100000</td>\n",
       "      <td>0.390000</td>\n",
       "      <td>0.090000</td>\n",
       "      <td>1.900000</td>\n",
       "      <td>0.070000</td>\n",
       "      <td>7.000000</td>\n",
       "      <td>22.000000</td>\n",
       "      <td>0.995600</td>\n",
       "      <td>3.210000</td>\n",
       "      <td>0.550000</td>\n",
       "      <td>9.500000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>7.900000</td>\n",
       "      <td>0.520000</td>\n",
       "      <td>0.260000</td>\n",
       "      <td>2.200000</td>\n",
       "      <td>0.079000</td>\n",
       "      <td>14.000000</td>\n",
       "      <td>38.000000</td>\n",
       "      <td>0.996750</td>\n",
       "      <td>3.310000</td>\n",
       "      <td>0.620000</td>\n",
       "      <td>10.200000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>9.200000</td>\n",
       "      <td>0.640000</td>\n",
       "      <td>0.420000</td>\n",
       "      <td>2.600000</td>\n",
       "      <td>0.090000</td>\n",
       "      <td>21.000000</td>\n",
       "      <td>62.000000</td>\n",
       "      <td>0.997835</td>\n",
       "      <td>3.400000</td>\n",
       "      <td>0.730000</td>\n",
       "      <td>11.100000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>15.900000</td>\n",
       "      <td>1.580000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>15.500000</td>\n",
       "      <td>0.611000</td>\n",
       "      <td>72.000000</td>\n",
       "      <td>289.000000</td>\n",
       "      <td>1.003690</td>\n",
       "      <td>4.010000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>14.900000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       fixedacidity  volatileacidity   citricacid  residualsugar    chlorides  \\\n",
       "count   1599.000000      1599.000000  1599.000000    1599.000000  1599.000000   \n",
       "mean       8.319637         0.527821     0.270976       2.538805     0.087467   \n",
       "std        1.741096         0.179060     0.194801       1.409928     0.047065   \n",
       "min        4.600000         0.120000     0.000000       0.900000     0.012000   \n",
       "25%        7.100000         0.390000     0.090000       1.900000     0.070000   \n",
       "50%        7.900000         0.520000     0.260000       2.200000     0.079000   \n",
       "75%        9.200000         0.640000     0.420000       2.600000     0.090000   \n",
       "max       15.900000         1.580000     1.000000      15.500000     0.611000   \n",
       "\n",
       "       freesulfurdioxide  totalsulfurdioxide      density           ph  \\\n",
       "count        1599.000000         1599.000000  1599.000000  1599.000000   \n",
       "mean           15.874922           46.467792     0.996747     3.311113   \n",
       "std            10.460157           32.895324     0.001887     0.154386   \n",
       "min             1.000000            6.000000     0.990070     2.740000   \n",
       "25%             7.000000           22.000000     0.995600     3.210000   \n",
       "50%            14.000000           38.000000     0.996750     3.310000   \n",
       "75%            21.000000           62.000000     0.997835     3.400000   \n",
       "max            72.000000          289.000000     1.003690     4.010000   \n",
       "\n",
       "         sulphates      alcohol      quality  \n",
       "count  1599.000000  1599.000000  1599.000000  \n",
       "mean      0.658149    10.422983     0.534709  \n",
       "std       0.169507     1.065668     0.498950  \n",
       "min       0.330000     8.400000     0.000000  \n",
       "25%       0.550000     9.500000     0.000000  \n",
       "50%       0.620000    10.200000     1.000000  \n",
       "75%       0.730000    11.100000     1.000000  \n",
       "max       2.000000    14.900000     1.000000  "
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wine.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Problem 1: K-NN (12 pts)**\n",
    "\n",
    "Train a K-Nearest Neighbor (k-NN) model with grid search and 10-fold cross-validation. Let the distance measure be Euclidean distance. Let the potential choices of be all odd numbers between 3 and 21 (inclusive). Training the model may take a couple of seconds.\n",
    "\n",
    "- What is the chosen k? What is the mean validation accuracy when the best k is chosen? \n",
    "- Suppose we have another wine, and we observe its features: **“7; 0.27; 0.36; 20.7; 0.045; 45; 170; 1.001; 3; 0.45; 8.8”**  (Input values are in the same order of the X variables, i.e., 'fixedacidity' = 7, volatileacidity = 0.27, etc.). \n",
    "    - Predict its quality using the selected k-NN model.\n",
    "    - Report ONLY the distances of the k Nearest Neighbors.\n",
    "    - Given the new sample, how many of its k Nearest Neighbors have the label of y = 1? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=10, estimator=KNeighborsClassifier(),\n",
       "             param_grid={'n_neighbors': range(3, 21, 2)})"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "# define function\n",
    "knn = KNeighborsClassifier()\n",
    "\n",
    "# define a list of parameters: odd numbers 3-25 inclusive\n",
    "param_knn = {'n_neighbors': range(3, 21, 2)}  # exactly the same as the input variable name. \n",
    "# range(i, j, s): integers from i to j, exclude j, increase by s\n",
    "\n",
    "#apply grid search\n",
    "grid_knn = GridSearchCV(knn, param_knn, cv = 10)\n",
    "grid_knn.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'n_neighbors': 13}\n",
      "0.671421568627451\n"
     ]
    }
   ],
   "source": [
    "# the best hyperparameter chosen:\n",
    "print(\"chosen K: \", grid_knn.best_params_)\n",
    "\n",
    "print(\"mean validation accuracy: \", grid_knn.best_score_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_new = [[7, 0.27, 0.36, 20.7, 0.045, 45, 170, 1.001, 3, 0.45, 8.8]]\n",
    "value = grid_knn.predict(X_new)\n",
    "#Predict its quality using the selected k-NN model\n",
    "#the quality would be array([0]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "distances (array([[26.4273, 28.3978, 29.7637, 31.0445, 31.7933, 32.8302, 34.205 ,\n",
      "        36.5136, 36.9853, 37.0005, 38.1209, 38.7989, 39.4244]]), array([[ 165,  545,  643,  343,  920,  778, 1013,  518,  682,  494,   24,\n",
      "        1123,   11]]))\n"
     ]
    }
   ],
   "source": [
    "#Report ONLY the distances of the k Nearest Neighbors.\n",
    "print(\"distances\", grid_knn.best_estimator_.kneighbors(X_new))\n",
    "\n",
    "#Given the new sample, how many of its k Nearest Neighbors have the label of y = 1?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Problem 2: Logistic regression with regularization (8 pts)**\n",
    "\n",
    "Train a logistic regression model with l2 regularization. \n",
    "- Report the coefficient values for fixedacidity and citricacid. (Hint: check code for data preparation step to find the order of variables.)\n",
    "- Obtain and report the True Positive Rate (TPR) and False Positive Rate (FPR) of the test set. \n",
    "- Suppose that we manually set the probability to 0.8. How would you expect the TPR change? How would you expect FPR change? Explain briefly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "#By default, l2 regularization is applied"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression()"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logreg = LogisticRegression()\n",
    "logreg.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.0016 -2.8923 -1.0658  0.0825 -0.9158  0.0255 -0.0204 -0.8692 -1.7845\n",
      "   2.0023  0.8651]]\n"
     ]
    }
   ],
   "source": [
    "print(logreg.coef_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fixedacidity: [ 0.0016 -2.8923 -1.0658  0.0825 -0.9158  0.0255 -0.0204 -0.8692 -1.7845\n",
      "  2.0023  0.8651]\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "index 2 is out of bounds for axis 0 with size 1",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/6y/8nbtlwpn2lv05l3yxc5kznnr0000gn/T/ipykernel_18594/999313854.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m      'totalsulfurdioxide','density', 'ph','sulphates', 'alcohol']\n\u001b[1;32m      4\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"fixedacidity:\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogreg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcoef_\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"citricacid:\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogreg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcoef_\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m: index 2 is out of bounds for axis 0 with size 1"
     ]
    }
   ],
   "source": [
    "#Report the coefficient values for fixedacidity and citricacid\n",
    "var = ['fixedacidity','volatileacidity','citricacid','residualsugar','chlorides','freesulfurdioxide',\\\n",
    "     'totalsulfurdioxide','density', 'ph','sulphates', 'alcohol']\n",
    "print(\"fixedacidity:\", logreg.coef_[0])\n",
    "print(\"citricacid:\", logreg.coef_[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Obtain and report the True Positive Rate (TPR) and False Positive Rate (FPR) of the test set.\n",
    "y_pred_test = logreg.predict(X_test)\n",
    "from sklearn.metrics import confusion_matrix\n",
    "my_matrix = confusion_matrix(y_test, y_pred_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TPR:  0.7465437788018433\n",
      "FPR:  0.32786885245901637\n"
     ]
    }
   ],
   "source": [
    "TP = my_matrix[1,1]\n",
    "TN = my_matrix[0,0]\n",
    "FP = my_matrix[0,1]\n",
    "FN = my_matrix[1,0]\n",
    "\n",
    "print(\"TPR: \", TP / (TP + FN))\n",
    "print(\"FPR: \", FP / (TN + FP))\n",
    "\n",
    "#from sklearn.metrics import precision_score, recall_score, accuracy_score, f1_score\n",
    "#print('Recall score:', recall_score(y_test, y_pred_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Suppose that we manually set the probability to 0.8. How would you expect the TPR change? How would you expect FPR change? Explain briefly.\n",
    "#if we manually set the proability to 0.8, that means all observations with a probability less than 0.8 will be classed as 0. The model will\n",
    "#be more conservativte, meaning it will classify more observations to 0. This will cause the TPR to decrease since there will be a lower ability\n",
    "# to correctly identify all positive instances out of actual positive instances. This will also cause FPR to decrease because out of all the actual\n",
    "#negatives, the model will correctly predict more as negative. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Problem 3: Performance Measure (14 pts)**\n",
    "\n",
    "Now, let’s use roc curves to visualize the performance of the previous models, and compare with a naive model.\n",
    "\n",
    "- We define the naive model be: predicting all records as y_pred = 1, regardless of their predictors. What would be its accuracy for the test set? \n",
    "- Generate roc curve of the logistic regression model using test set.  \n",
    "- In the same figure, generate roc curve of the optimal kNN model using the test set. \n",
    "    - *Hint 1: By including the visualization syntax in two consecutive lines, you will obtain overlaied figure).* \n",
    "    - *Hint 2: In KNN, you will use the same syntax as in logistic regression to obtain the predicted probability, i.e., pi-hat*\n",
    "- In the same figure, generate roc curve of the naive model. Label axis accordingly. \n",
    "    - *Hint: you should be able to complete this task using syntax learned in class. You may want to check Lecture 1's Coding File*\n",
    "- Based on the figure, which model has a better performance? Based on the comparison, are we able to conclude which model is better? Do you have any concerns when conducting this comparison?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.4575]\n",
      "[0.5425]\n"
     ]
    }
   ],
   "source": [
    "#What would be its accuracy for the test set?\n",
    "Acc_Zeros = np.sum(0 == y_test)/y_test.shape \n",
    "print(Acc_Zeros)\n",
    "\n",
    "Acc_Ones = np.sum(1 == y_test)/y_test.shape \n",
    "print(Acc_Ones)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0, 0.5, 'TPR')"
      ]
     },
     "execution_count": 140,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Generate roc curve of the logistic regression model using test set.\n",
    "from sklearn.metrics import roc_curve\n",
    "pred_prob = logreg.predict_proba(X_test)[:,1] #prob_y=1\n",
    "logreg.predict_proba(X_test)[ : , 1 ]\n",
    "FPR, TPR, threshold = roc_curve(y_test, pred_prob)\n",
    "plt.plot(FPR, TPR)\n",
    "plt.xlabel(\"FPR\")\n",
    "plt.ylabel(\"TPR\")\n",
    "\n",
    "#In the same figure, generate roc curve of the optimal kNN model using the test set\n",
    "from sklearn.metrics import roc_curve\n",
    "pred_prob = grid_knn.predict_proba(X_test)[:,1] #prob_y=1\n",
    "grid_knn.predict_proba(X_test)[ : , 1 ]\n",
    "FPR, TPR, threshold = roc_curve(y_test, pred_prob)\n",
    "plt.plot(FPR, TPR)\n",
    "plt.xlabel(\"FPR\")\n",
    "plt.ylabel(\"TPR\")\n",
    "\n",
    "#In the same figure, generate roc curve of the naive model. Label axis accordingly\n",
    "from sklearn.metrics import roc_curve\n",
    "pred_prob = logreg.predict_proba(X_train)[:,1] #prob_y=1\n",
    "logreg.predict_proba(X_train)[ : , 1 ]\n",
    "FPR, TPR, threshold = roc_curve(y_train, pred_prob)\n",
    "plt.plot(FPR, TPR)\n",
    "plt.xlabel(\"FPR\")\n",
    "plt.ylabel(\"TPR\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Based on the figure, which model has a better performance? Based on the comparison, are we able to conclude which model is better? \n",
    "#Do you have any concerns when conducting this comparison?\n",
    "#based on the figure, the ROC curve generated by the naive model has the best performance. This is because the naive model is based on the training data \n",
    "#which is not an unbiased performance measure. My concerns when making this comparison is that using the training data is biased because it is\n",
    "#the same data that is used to estimate the parameters "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7\n"
     ]
    }
   ],
   "source": [
    "def great(a, b):\n",
    "    while a != 0 and b != 0:\n",
    "        if a > b:\n",
    "            a = a - b\n",
    "        else:\n",
    "            b = b - a\n",
    "    return max(a, b)\n",
    "\n",
    "result = great(21, 35)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "34\n"
     ]
    }
   ],
   "source": [
    "j = 1\n",
    "k = 2\n",
    "\n",
    "for i in range(3):\n",
    "    j += k\n",
    "    k += j\n",
    "\n",
    "print(k)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a\n",
      "adef\n"
     ]
    }
   ],
   "source": [
    "strings = [\"a\", \"bc\", \"def\", \"ghij\"]\n",
    "output = \"\"\n",
    "for x in strings:\n",
    "    if len(x) % 2 == 0: \n",
    "        continue\n",
    "    output += x\n",
    "    print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "output"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
