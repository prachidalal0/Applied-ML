{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lecture 8.2 SVM and Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "# import necessary libraries and specify that graphs should be plotted inline. \n",
    "\n",
    "# from sklearn.tree import DecisionTreeClassifier\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading Data for Scikit-Learn Datasets\n",
    "In today's practice, we will use two datasets: the cancer dataset, and the iris dataset. Both are sklearn-embedded datasets. Run cell below to check details for cancer dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Key Elements dict_keys(['data', 'target', 'frame', 'target_names', 'DESCR', 'feature_names', 'filename'])\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import load_breast_cancer # Loading all info of cancer dataset\n",
    "cancer = load_breast_cancer()\n",
    "print(\"Key Elements\", cancer.keys())\n",
    "# cancer.target_names\n",
    "# cancer.feature_names\n",
    "# print(cancer.DESCR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Support Vector Machine\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on whether the separator is linear (i.e., using kernel function or not), we classify the SVM approach into two types: linear SVM and kernel SVM. These two approaches are realized through different syntax in Scikit-Learn. \n",
    "\n",
    "Recall that we also mentioned hard/soft-SVM, based on whether the classifier allows for noisy data points. This difference will be accommodated using different values of hyperparameter C. We set C to a very large number as an approximation of hard-margin SVM.\n",
    "\n",
    "### Linear SVM\n",
    "For a baisc Linear SVM classifier, we use syntax:\n",
    "**<center>sklearn.svm.LinearSVC()</center>**\n",
    "- **C:** Hyperparameter of how acceptable the model is for margin violations. Smaller C indicates more acceptability. Default value is 1.\n",
    "- Set random state for technical reasons.\n",
    "\n",
    "#### Practice\n",
    "- Load cancer data (sklearn.datasets.load_breast_cancer), use all variables (except the target) as predictors, split the data.\n",
    "- Train a linear SVC, leave all settings as default. \n",
    "    - What is the training and test score?\n",
    "- Train a linear SVC with grid search and 5-fold cross validation. *(Hint: You may want to set n_jobs = 2 as an input parameter for GridSearchCV. This saves some time for computation)*\n",
    "    - Let choices of C be: [0.001, 0.01, 0.1, 1, 10, 100, 100000]. \n",
    "    - What is the the best C? \n",
    "    - Under this case, what is the training and test score? \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading Data\n",
    "from sklearn.datasets import load_breast_cancer\n",
    "cancer = load_breast_cancer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Splitting Data\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(cancer.data, cancer.target, random_state = 42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.965034965034965"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Linear SVC \n",
    "from sklearn.svm import LinearSVC\n",
    "\n",
    "lr_svc = LinearSVC(random_state = 0) # default C=1\n",
    "lr_svc.fit(X_train, y_train)\n",
    "\n",
    "lr_svc.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# CPU available\n",
    "import os\n",
    "n_cpu = os.cpu_count()\n",
    "n_cpu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GridSearchCV(estimator=LinearSVC(random_state=22), n_jobs=2,\n",
       "             param_grid={'C': [0.001, 0.01, 0.1, 1, 10, 100, 100000]})"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Linear SVC with GridSearchCV\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# Define Function\n",
    "linear_svc = LinearSVC(random_state = 22) # must specify random state here\n",
    "\n",
    "# Define a list of hyperparameters\n",
    "params_svc = {'C': [0.001, 0.01, 0.1, 1, 10, 100, 100000]  }\n",
    "\n",
    "grid_lrsvc = GridSearchCV(linear_svc, params_svc, n_jobs = 2)\n",
    "\n",
    "grid_lrsvc.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'C': 0.001}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grid_lrsvc.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SVM with Kernel Functions\n",
    "In most cases, SVM comes together with kernel functions, so that the classifier can handle non-linear separable cases. For implementation, we use syntax:\n",
    "**<center>sklearn.svm.SVC()</center>**\n",
    "- First, be aware that the last three letters, SVC, are capitalized.\n",
    "- **C:** used to specify how acceptable for margin violations. Same as linear case.\n",
    "- **kernel:** used to specify the kernel function. Choose from {'linear', 'poly', 'rbf', 'sigmoid', 'recomputed'}. These are different kernel functions. We introduced only two of them (i.e., poly and rbf). The default value is 'rbf'.\n",
    "- **degree:** used when kernel='poly', to specify the polynomial degree. Default value = 3.\n",
    "- **gamma:** used when kernel = 'rbf', we can set it manually. Default is relevant to feature number and feature variations.\n",
    "- Set random state for technical reasons.\n",
    "\n",
    "To explore other details, check https://scikit-learn.org/stable/modules/generated/sklearn.svm.SVC.html\n",
    "\n",
    "#### Practice\n",
    "- Use the same cancer data and training-test splittion. Train an SVM model, let kernel function be rbf, leave other parameters and hyperparameters as default. What is the accuracy for training set and test set?\n",
    "- Train an SVM model, let kernel function be rbf. \n",
    "    - Apply grid search with 5-fold CV. Let choices of C be: [0.001, 0.01, 0.1, 1, 10, 100, 10000]. \n",
    "    - Let choices of gamma be: [0.0001, 0.001,0.001,0.1,1,10]. \n",
    "    - Which model is the best? \n",
    "    - What is the performance of the training and test set?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.951048951048951"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.svm import SVC\n",
    "k_svc = SVC(random_state = 0, kernel = 'rbf')\n",
    "k_svc.fit(X_train, y_train)\n",
    "k_svc.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=5, estimator=SVC(random_state=0), n_jobs=2,\n",
       "             param_grid={'C': [0.001, 0.01, 0.1, 1, 10, 100, 10000],\n",
       "                         'gamma': [0.0001, 0.001, 0.001, 0.1, 1, 10]})"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.svm import SVC\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "# Define Function\n",
    "base_svc = SVC(random_state = 0, kernel = 'rbf') # if poly kernel, then kernel = 'poly'\n",
    "\n",
    "#define a list of parameters\n",
    "param_svc_kernel = {'C':   [0.001, 0.01, 0.1, 1, 10, 100, 10000]     ,\n",
    "                    'gamma':  [0.0001, 0.001,0.001,0.1,1,10]  } # C = 10,000 mimics hard-margin SVM\n",
    "\n",
    "#apply grid search\n",
    "grid_ksvc = GridSearchCV(base_svc, param_svc_kernel, cv = 5, n_jobs=2)\n",
    "\n",
    "grid_ksvc.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'C': 10, 'gamma': 0.0001}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grid_ksvc.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Naive Bayes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Naive Bayes can be implemented in several ways. We discuss two specific cases in today's class: (1) If all predictors are categorical, and (2) If some or all predictors are continuous.\n",
    "\n",
    "#### Categorical Features\n",
    "If all predictors are categorical, use syntax:\n",
    "\n",
    "**<center>sklearn.naive_bayes.CategoricalNB()</center>**\n",
    "- alpha: A smoothing factor. The default value is 1. To get the same result as manually calculated, set alpha = 0.\n",
    "\n",
    "#### Continuous Features\n",
    "If all predictors are continuous, we would need to use one of the methods below:\n",
    "1. Bin the continuous variable first, then use the previously mentioned syntax, sklearn.naive_bayes.CategoricalNB(). *We do not discuss on this method in here.*\n",
    "\n",
    "2. Assume the data follows a normal distribution. Then we can use the syntax below: \n",
    "\n",
    "**<center>sklearn.naive_bayes.GaussianNB()</center>**\n",
    "\n",
    "For both models (i.e., categorical NB and Gaussian NB), we can obtain predicted probability for each class using .predict_proba. \n",
    "\n",
    "#### Practice 1\n",
    "Replicate the result of the in-class practice (i.e., firm report example, probability = 0.47).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plug in data: \n",
    "## Charges: =1 if yes, =0 if no.\n",
    "## Size: =1 if large, =0 if small\n",
    "## Y: =1 if T, =0 if F\n",
    "X = np.array([[1,0], [0,0], [0,1], [0, 1], [0,0], [0,0], [1,0], [1,1], [0,1], [1,1]])\n",
    "Y = np.array([1,1,1,1,1,1,0,0,0,0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import CategoricalNB\n",
    "\n",
    "\n",
    "\n",
    "# New Record: Yes, Small\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Practice 2\n",
    "Using the iris data, train a Naive Bayes model. Assume variables are normally distributed.\n",
    "- Split the data into training and test\n",
    "- Train the model on the training set (Use Gaussian NB)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_iris\n",
    "iris = load_iris() \n",
    "# load the complete data information in. It consists both data and descriptive info.\n",
    "iris.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Splitting data\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(iris.data, iris.target, random_state=22)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training the Model\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
